{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMiTOyiugrlGlf/vy0gTWsC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atharvadesai1/BE-Project-Codes/blob/main/liver_tumor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOnqGc4Sqku4",
        "outputId": "46f90965-8221-4d66-8abe-e8c040608c4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  /content/drive/MyDrive/liver_segmentation_dataset.zip\n",
            "  inflating: /content/dataset/segmentations/segmentation-0.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-1.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-10.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-100.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-101.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-102.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-103.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-104.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-105.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-106.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-107.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-108.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-109.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-11.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-110.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-111.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-112.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-113.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-114.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-115.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-116.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-117.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-118.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-119.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-12.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-120.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-121.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-122.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-123.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-124.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-125.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-126.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-127.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-128.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-129.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-13.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-130.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-14.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-15.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-16.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-17.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-18.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-19.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-2.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-20.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-21.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-22.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-23.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-24.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-25.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-26.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-27.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-28.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-29.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-3.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-30.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-31.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-32.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-33.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-34.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-35.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-36.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-37.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-38.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-39.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-4.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-40.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-41.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-42.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-43.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-44.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-45.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-46.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-47.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-48.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-49.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-5.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-50.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-51.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-52.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-53.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-54.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-55.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-56.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-57.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-58.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-59.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-6.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-60.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-61.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-62.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-63.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-64.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-65.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-66.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-67.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-68.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-69.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-7.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-70.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-71.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-72.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-73.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-74.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-75.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-76.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-77.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-78.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-79.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-8.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-80.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-81.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-82.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-83.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-84.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-85.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-86.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-87.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-88.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-89.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-9.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-90.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-91.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-92.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-93.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-94.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-95.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-96.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-97.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-98.nii  \n",
            "  inflating: /content/dataset/segmentations/segmentation-99.nii  \n",
            "  inflating: /content/dataset/volume_pt1/volume-0.nii  \n",
            "  inflating: /content/dataset/volume_pt1/volume-1.nii  \n",
            "  inflating: /content/dataset/volume_pt1/volume-10.nii  \n",
            "  inflating: /content/dataset/volume_pt1/volume-2.nii  \n",
            "  inflating: /content/dataset/volume_pt1/volume-3.nii  \n",
            "  inflating: /content/dataset/volume_pt1/volume-4.nii  \n",
            "  inflating: /content/dataset/volume_pt1/volume-5.nii  \n",
            "  inflating: /content/dataset/volume_pt1/volume-6.nii  \n",
            "  inflating: /content/dataset/volume_pt1/volume-7.nii  \n",
            "  inflating: /content/dataset/volume_pt1/volume-8.nii  "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!unzip \"/content/drive/MyDrive/liver_segmentation_dataset.zip\" -d \"/content/dataset\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "import glob\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "9pdTvZaWq_wr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(ct_path, mask_path):\n",
        "    ct_scans = []\n",
        "    masks = []\n",
        "\n",
        "    for ct_file in sorted(glob.glob(ct_path + \"/*.nii\")):\n",
        "        mask_file = os.path.join(mask_path, os.path.basename(ct_file).replace(\"volume\", \"segmentation\"))\n",
        "\n",
        "        if os.path.exists(mask_file):\n",
        "            ct_scans.append(ct_file)\n",
        "            masks.append(mask_file)\n",
        "\n",
        "    return ct_scans, masks\n",
        "\n",
        "ct_scans, masks = load_data(\"/content/dataset/ct_scans\", \"/content/dataset/masks\")"
      ],
      "metadata": {
        "id": "J1pUGmdWrnvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(image, target_size=(256, 256)):\n",
        "    image = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)\n",
        "    image = cv2.resize(image, target_size)\n",
        "    image = image / 255.0\n",
        "    return np.expand_dims(image, axis=-1)\n",
        "\n",
        "def preprocess_mask(mask, target_size=(256, 256)):\n",
        "    mask = cv2.resize(mask, target_size)\n",
        "    mask = (mask > 0).astype(np.float32)\n",
        "    return np.expand_dims(mask, axis=-1)"
      ],
      "metadata": {
        "id": "FnEhDmv3rwuh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, ct_paths, mask_paths, batch_size=8, target_size=(256, 256), shuffle=True):\n",
        "        self.ct_paths = ct_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.target_size = target_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.ct_paths) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        ct_batch = [self.ct_paths[k] for k in indexes]\n",
        "        mask_batch = [self.mask_paths[k] for k in indexes]\n",
        "\n",
        "        X = np.zeros((self.batch_size, *self.target_size, 1), dtype=np.float32)\n",
        "        y = np.zeros((self.batch_size, *self.target_size, 1), dtype=np.float32)\n",
        "\n",
        "        for i, (ct_path, mask_path) in enumerate(zip(ct_batch, mask_batch)):\n",
        "            ct = nib.load(ct_path).get_fdata()\n",
        "            mask = nib.load(mask_path).get_fdata()\n",
        "\n",
        "            X[i,] = preprocess_image(ct)\n",
        "            y[i,] = preprocess_mask(mask)\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.ct_paths))\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)"
      ],
      "metadata": {
        "id": "qPPr6Bsqs6fz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unet_model(input_size=(256, 256, 1)):\n",
        "    inputs = Input(input_size)\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
        "    conv1 = Conv2D(64, 3, activation='relu', padding='same')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(pool1)\n",
        "    conv2 = Conv2D(128, 3, activation='relu', padding='same')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(pool2)\n",
        "    conv3 = Conv2D(256, 3, activation='relu', padding='same')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    # Bottleneck\n",
        "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(pool3)\n",
        "    conv4 = Conv2D(512, 3, activation='relu', padding='same')(conv4)\n",
        "\n",
        "    # Decoder\n",
        "    up5 = UpSampling2D(size=(2, 2))(conv4)\n",
        "    merge5 = concatenate([conv3, up5], axis=3)\n",
        "    conv5 = Conv2D(256, 3, activation='relu', padding='same')(merge5)\n",
        "    conv5 = Conv2D(256, 3, activation='relu', padding='same')(conv5)\n",
        "\n",
        "    up6 = UpSampling2D(size=(2, 2))(conv5)\n",
        "    merge6 = concatenate([conv2, up6], axis=3)\n",
        "    conv6 = Conv2D(128, 3, activation='relu', padding='same')(merge6)\n",
        "    conv6 = Conv2D(128, 3, activation='relu', padding='same')(conv6)\n",
        "\n",
        "    up7 = UpSampling2D(size=(2, 2))(conv6)\n",
        "    merge7 = concatenate([conv1, up7], axis=3)\n",
        "    conv7 = Conv2D(64, 3, activation='relu', padding='same')(merge7)\n",
        "    conv7 = Conv2D(64, 3, activation='relu', padding='same')(conv7)\n",
        "\n",
        "    outputs = Conv2D(1, 1, activation='sigmoid')(conv7)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ],
      "metadata": {
        "id": "PVM51qrBs9X0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = unet_model()\n",
        "model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "train_ct, val_ct, train_mask, val_mask = train_test_split(ct_scans, masks, test_size=0.2)\n",
        "\n",
        "train_gen = DataGenerator(train_ct, train_mask)\n",
        "val_gen = DataGenerator(val_ct, val_mask)\n",
        "\n",
        "checkpoint = ModelCheckpoint('liver_tumor_segmentation.h5', monitor='val_accuracy', save_best_only=True)\n",
        "early_stop = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(train_gen, validation_data=val_gen, epochs=50, callbacks=[checkpoint, early_stop])"
      ],
      "metadata": {
        "id": "t0OWXwPJtIMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_cam(model, img, layer_name=\"conv5_block3_out\"):\n",
        "    grad_model = Model(inputs=model.inputs, outputs=[model.get_layer(layer_name).output, model.output])\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_output, pred = grad_model(np.expand_dims(img, axis=0))\n",
        "        pred_class = tf.argmax(pred[0])\n",
        "        loss = pred[:, pred_class]\n",
        "\n",
        "    grads = tape.gradient(loss, conv_output)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    conv_output = conv_output[0]\n",
        "    heatmap = conv_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "\n",
        "    return heatmap.numpy()\n",
        "\n",
        "# Example Grad-CAM visualization\n",
        "sample_img = preprocess_image(nib.load(val_ct[0]).get_fdata())\n",
        "heatmap = grad_cam(model, sample_img)\n",
        "\n",
        "plt.imshow(sample_img[:, :, 0], cmap='gray')\n",
        "plt.imshow(cv2.resize(heatmap, (256, 256)), alpha=0.5, cmap='jet')\n",
        "plt.title(\"Grad-CAM Explanation\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tLdqtiyVtI4C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}